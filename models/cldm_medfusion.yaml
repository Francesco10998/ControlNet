model:
  target: medfusion.models.DiffusionModel
  params:
    linear_start: 0.0001
    linear_end: 0.02
    timesteps: 1000
    image_size: 256
    channels: 3
    num_heads: 8
    attention_resolutions: [32, 16, 8]
    num_res_blocks: 2
    channel_mult: [1, 2, 4, 8]
    dropout: 0.1
    cond_stage_key: "label"
    first_stage_key: "vae"
    monitor: val/loss_simple_ema

    first_stage_config:
      target: medfusion.models.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_channels: 3
          ch: 128
          ch_mult: [1, 2, 4, 4]
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: medfusion.modules.encoders.LabelEmbedder
      params:
        embed_dim: 768

data:
  train:
    target: medfusion.data.SimpleDataset2D
    params:
      data_dir: /path/to/train/data
      image_size: 256
      channels: 3
  val:
    target: medfusion.data.SimpleDataset2D
    params:
      data_dir: /path/to/val/data
      image_size: 256
      channels: 3

trainer:
  gpus: 1
  max_epochs: 100
  precision: 16
  accumulate_grad_batches: 2
  gradient_clip_val: 1.0
  val_check_interval: 1.0

callbacks:
  - target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      monitor: val/loss
      save_top_k: 3
      mode: min

logging:
  target: pytorch_lightning.loggers.TensorBoardLogger
  params:
    save_dir: /path/to/logs
    name: medfusion_experiment
